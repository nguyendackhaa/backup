#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
C√†o d·ªØ li·ªáu t·ª´ website Fotek.com.tw
Ph√°t tri·ªÉn b·ªüi: Assistant AI
Ch·ª©c nƒÉng: C√†o d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ Fotek.com.tw v·ªõi ƒëa lu·ªìng v√† AI Gemini
"""

import requests
import json
import os
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, urlparse
from pathlib import Path
import pandas as pd
from PIL import Image, ImageDraw
import io
import base64
import re
from typing import List, Dict, Optional, Tuple
import logging

# Th∆∞ vi·ªán cho web scraping
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

# Th∆∞ vi·ªán cho AI Gemini
import google.generativeai as genai

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('fotek_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FotekScraper:
    """Class ch√≠nh cho vi·ªác c√†o d·ªØ li·ªáu t·ª´ Fotek.com.tw"""
    
    def __init__(self, gemini_api_key: str = None, max_workers: int = 5):
        """
        Kh·ªüi t·∫°o FotekScraper
        
        Args:
            gemini_api_key: API key cho Google Gemini AI
            max_workers: S·ªë l∆∞·ª£ng lu·ªìng t·ªëi ƒëa ƒë·ªÉ x·ª≠ l√Ω song song
        """
        self.base_url = "https://www.fotek.com.tw"
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # C·∫•u h√¨nh Selenium WebDriver
        self.chrome_options = Options()
        self.chrome_options.add_argument('--headless')
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        self.chrome_options.add_argument('--disable-gpu')
        self.chrome_options.add_argument('--window-size=1920,1080')
        self.chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        # C·∫•u h√¨nh Gemini AI
        if gemini_api_key:
            genai.configure(api_key=gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
        else:
            self.gemini_model = None
            logger.warning("Kh√¥ng c√≥ API key cho Gemini AI. Ch·ª©c nƒÉng d·ªãch v√† tr√≠ch xu·∫•t s·∫Ω b·ªã v√¥ hi·ªáu h√≥a.")
        
        # T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ
        self.output_dir = Path("fotek_data")
        self.images_dir = self.output_dir / "images"
        self.specs_dir = self.output_dir / "specifications"
        self.excel_dir = self.output_dir / "excel"
        
        for dir_path in [self.output_dir, self.images_dir, self.specs_dir, self.excel_dir]:
            dir_path.mkdir(exist_ok=True)
        
        logger.info("FotekScraper ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng")
    
    def get_driver(self) -> webdriver.Chrome:
        """T·∫°o m·ªôt WebDriver Chrome m·ªõi"""
        try:
            driver = webdriver.Chrome(options=self.chrome_options)
            driver.implicitly_wait(10)
            return driver
        except Exception as e:
            logger.error(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o WebDriver: {e}")
            raise
    
    def close_driver(self, driver: webdriver.Chrome):
        """ƒê√≥ng WebDriver"""
        try:
            driver.quit()
        except Exception as e:
            logger.warning(f"L·ªói khi ƒë√≥ng WebDriver: {e}")
    
    def safe_request(self, url: str, timeout: int = 30) -> Optional[requests.Response]:
        """Th·ª±c hi·ªán request an to√†n v·ªõi retry"""
        for attempt in range(3):
            try:
                response = self.session.get(url, timeout=timeout)
                response.raise_for_status()
                return response
            except requests.RequestException as e:
                logger.warning(f"L·ªói request (l·∫ßn {attempt + 1}): {e}")
                if attempt == 2:
                    return None
                time.sleep(2 ** attempt)
        return None
    
    def extract_series_links(self, category_url: str) -> List[Dict[str, str]]:
        """
        Tr√≠ch xu·∫•t c√°c link series s·∫£n ph·∫©m t·ª´ trang danh m·ª•c
        
        Args:
            category_url: URL c·ªßa trang danh m·ª•c
            
        Returns:
            List c√°c dict ch·ª©a th√¥ng tin series (name, url, image)
        """
        logger.info(f"B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t series t·ª´: {category_url}")
        
        response = self.safe_request(category_url)
        if not response:
            logger.error(f"Kh√¥ng th·ªÉ truy c·∫≠p trang danh m·ª•c: {category_url}")
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        series_list = []
        
        # T√¨m section ch·ª©a c√°c series
        bg_light_section = soup.find('section', class_='bg-light')
        if not bg_light_section:
            logger.error("Kh√¥ng t√¨m th·∫•y section bg-light")
            return []
        
        # T√¨m t·∫•t c·∫£ c√°c card series
        cards = bg_light_section.find_all('div', class_='card mb-4')
        logger.info(f"T√¨m th·∫•y {len(cards)} series cards")
        
        for card in cards:
            try:
                # Tr√≠ch xu·∫•t t√™n series
                title_element = card.find('h4')
                series_name = title_element.text.strip() if title_element else "Unknown Series"
                
                # Tr√≠ch xu·∫•t link
                link_element = card.find('a', class_='stretched-link')
                if not link_element or not link_element.get('href'):
                    continue
                
                series_url = urljoin(self.base_url, link_element['href'])
                
                # Tr√≠ch xu·∫•t ·∫£nh series
                img_div = card.find('div', class_='box-img')
                series_image = ""
                if img_div and img_div.get('style'):
                    style = img_div['style']
                    match = re.search(r"background-image:url\('([^']+)'\)", style)
                    if match:
                        series_image = urljoin(self.base_url, match.group(1))
                
                # Tr√≠ch xu·∫•t m√¥ t·∫£
                description_element = card.find('p', class_='txt-l3')
                description = description_element.text.strip() if description_element else ""
                
                series_info = {
                    'name': series_name,
                    'url': series_url,
                    'image': series_image,
                    'description': description
                }
                
                series_list.append(series_info)
                logger.info(f"ƒê√£ tr√≠ch xu·∫•t series: {series_name}")
                
            except Exception as e:
                logger.error(f"L·ªói khi tr√≠ch xu·∫•t th√¥ng tin series: {e}")
                continue
        
        logger.info(f"Ho√†n th√†nh tr√≠ch xu·∫•t {len(series_list)} series")
        return series_list
    
    def extract_products_from_series(self, series_url: str) -> List[Dict[str, str]]:
        """
        Tr√≠ch xu·∫•t danh s√°ch s·∫£n ph·∫©m t·ª´ trang series
        
        Args:
            series_url: URL c·ªßa trang series
            
        Returns:
            List c√°c dict ch·ª©a th√¥ng tin s·∫£n ph·∫©m c∆° b·∫£n
        """
        logger.info(f"B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t s·∫£n ph·∫©m t·ª´ series: {series_url}")
        
        response = self.safe_request(series_url)
        if not response:
            logger.error(f"Kh√¥ng th·ªÉ truy c·∫≠p trang series: {series_url}")
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        products_list = []
        
        # T√¨m container ch·ª©a c√°c s·∫£n ph·∫©m
        container = soup.find('div', class_='container')
        if not container:
            logger.error("Kh√¥ng t√¨m th·∫•y container s·∫£n ph·∫©m")
            return []
        
        # T√¨m t·∫•t c·∫£ c√°c item-type (nh√≥m s·∫£n ph·∫©m)
        item_types = container.find_all('div', class_='item-type')
        logger.info(f"T√¨m th·∫•y {len(item_types)} nh√≥m s·∫£n ph·∫©m")
        
        for item_type in item_types:
            try:
                # L·∫•y t√™n nh√≥m s·∫£n ph·∫©m
                group_name_element = item_type.find('h5')
                group_name = group_name_element.text.strip() if group_name_element else "Unknown Group"
                
                # L·∫•y ·∫£nh nh√≥m
                img_div = item_type.find('div', class_='box-img')
                group_image = ""
                if img_div and img_div.get('style'):
                    style = img_div['style']
                    match = re.search(r"background-image:url\('([^']+)'\)", style)
                    if match:
                        group_image = urljoin(self.base_url, match.group(1))
                
                # T√¨m t·∫•t c·∫£ s·∫£n ph·∫©m trong nh√≥m
                product_items = item_type.find_all('li')
                
                for li in product_items:
                    spans = li.find_all('span')
                    if len(spans) >= 3:
                        product_code = spans[0].text.strip()
                        features = spans[1].text.strip()
                        
                        # T√¨m link "View specs"
                        specs_link_element = spans[2].find('a')
                        if specs_link_element and specs_link_element.get('href'):
                            product_url = urljoin(self.base_url, specs_link_element['href'])
                            
                            product_info = {
                                'code': product_code,
                                'features': features,
                                'url': product_url,
                                'group_name': group_name,
                                'group_image': group_image,
                                'series_url': series_url
                            }
                            
                            products_list.append(product_info)
                            logger.debug(f"ƒê√£ tr√≠ch xu·∫•t s·∫£n ph·∫©m: {product_code}")
                
            except Exception as e:
                logger.error(f"L·ªói khi tr√≠ch xu·∫•t nh√≥m s·∫£n ph·∫©m: {e}")
                continue
        
        logger.info(f"Ho√†n th√†nh tr√≠ch xu·∫•t {len(products_list)} s·∫£n ph·∫©m t·ª´ series")
        return products_list
    
    def extract_product_details(self, product_url: str, product_code: str) -> Dict:
        """
        Tr√≠ch xu·∫•t chi ti·∫øt s·∫£n ph·∫©m t·ª´ trang s·∫£n ph·∫©m
        
        Args:
            product_url: URL c·ªßa trang s·∫£n ph·∫©m
            product_code: M√£ s·∫£n ph·∫©m
            
        Returns:
            Dict ch·ª©a th√¥ng tin chi ti·∫øt s·∫£n ph·∫©m
        """
        logger.info(f"B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t chi ti·∫øt s·∫£n ph·∫©m: {product_code}")
        
        driver = self.get_driver()
        try:
            driver.get(product_url)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # L·∫•y HTML sau khi JavaScript ƒë√£ load
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            product_details = {
                'code': product_code,
                'url': product_url,
                'name': '',
                'name_vietnamese': '',
                'images': {
                    'product': [],
                    'specifications': '',
                    'wiring_diagram': '',
                    'dimensions': ''
                },
                'specifications_table': '',
                'specifications_raw': ''
            }
            
            # Tr√≠ch xu·∫•t t√™n s·∫£n ph·∫©m
            title_element = soup.find('p', class_='title-card')
            if title_element:
                product_details['name'] = title_element.text.strip()
            
            # Tr√≠ch xu·∫•t ·∫£nh s·∫£n ph·∫©m ch√≠nh
            gallery = soup.find('div', class_='gallery')
            if gallery:
                product_images = []
                for item in gallery.find_all('div', class_='item'):
                    link = item.find('a')
                    if link and link.get('href'):
                        img_url = urljoin(self.base_url, link['href'])
                        product_images.append(img_url)
                product_details['images']['product'] = product_images
            
            # Tr√≠ch xu·∫•t th√¥ng tin t·ª´ c√°c tab
            tab_content = soup.find('div', class_='tab-content')
            if tab_content:
                # Tab specifications (tab1)
                tab1 = tab_content.find('div', id='tab1')
                if tab1:
                    img_element = tab1.find('img')
                    if img_element and img_element.get('src'):
                        product_details['images']['specifications'] = urljoin(self.base_url, img_element['src'])
                
                # Tab wiring diagram (tab2)
                tab2 = tab_content.find('div', id='tab2')
                if tab2:
                    img_element = tab2.find('img')
                    if img_element and img_element.get('src'):
                        product_details['images']['wiring_diagram'] = urljoin(self.base_url, img_element['src'])
                
                # Tab dimensions (tab3)
                tab3 = tab_content.find('div', id='tab3')
                if tab3:
                    img_element = tab3.find('img')
                    if img_element and img_element.get('src'):
                        product_details['images']['dimensions'] = urljoin(self.base_url, img_element['src'])
            
            logger.info(f"Ho√†n th√†nh tr√≠ch xu·∫•t chi ti·∫øt s·∫£n ph·∫©m: {product_code}")
            return product_details
            
        except Exception as e:
            logger.error(f"L·ªói khi tr√≠ch xu·∫•t chi ti·∫øt s·∫£n ph·∫©m {product_code}: {e}")
            return {}
        finally:
            self.close_driver(driver)
    
    def translate_with_gemini(self, text: str, target_language: str = "Vietnamese") -> str:
        """
        D·ªãch vƒÉn b·∫£n b·∫±ng Gemini AI
        
        Args:
            text: VƒÉn b·∫£n c·∫ßn d·ªãch
            target_language: Ng√¥n ng·ªØ ƒë√≠ch
            
        Returns:
            VƒÉn b·∫£n ƒë√£ d·ªãch
        """
        if not self.gemini_model:
            logger.warning("Gemini AI kh√¥ng kh·∫£ d·ª•ng")
            return text
        
        try:
            prompt = f"Translate the following text to {target_language}: {text}"
            response = self.gemini_model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error(f"L·ªói khi d·ªãch vƒÉn b·∫£n: {e}")
            return text
    
    def extract_specs_from_image_with_gemini(self, image_url: str, product_code: str) -> str:
        """
        Tr√≠ch xu·∫•t th√¥ng s·ªë k·ªπ thu·∫≠t t·ª´ ·∫£nh b·∫±ng Gemini AI v√† t·∫°o b·∫£ng HTML
        
        Args:
            image_url: URL c·ªßa ·∫£nh th√¥ng s·ªë k·ªπ thu·∫≠t
            product_code: M√£ s·∫£n ph·∫©m
            
        Returns:
            HTML table ch·ª©a th√¥ng s·ªë k·ªπ thu·∫≠t
        """
        if not self.gemini_model:
            logger.warning("Gemini AI kh√¥ng kh·∫£ d·ª•ng")
            return ""
        
        try:
            # T·∫£i ·∫£nh
            response = self.safe_request(image_url)
            if not response:
                return ""
            
            # Chuy·ªÉn ·∫£nh th√†nh base64
            image_data = base64.b64encode(response.content).decode()
            
            prompt = f"""
            Ph√¢n t√≠ch ·∫£nh th√¥ng s·ªë k·ªπ thu·∫≠t n√†y v√† tr√≠ch xu·∫•t t·∫•t c·∫£ th√¥ng tin k·ªπ thu·∫≠t.
            D·ªãch sang ti·∫øng Vi·ªát v√† t·∫°o b·∫£ng HTML theo ƒë·ªãnh d·∫°ng sau:
            
            <table id="specifications" border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; font-family: Arial; width: 100%;">
            <thead>
            <tr style="background-color: #f2f2f2;">
            <th>Th√¥ng s·ªë</th>
            <th>Gi√° tr·ªã</th>
            </tr>
            </thead>
            <tbody>
            <tr><td style="font-weight: bold;">M√£ s·∫£n ph·∫©m</td><td>{product_code}</td></tr>
            [Th√™m c√°c th√¥ng s·ªë k·ªπ thu·∫≠t kh√°c ·ªü ƒë√¢y]
            <tr><td style="font-weight: bold;">Copyright</td><td>Haiphongtech.vn</td></tr>
            </tbody>
            </table>
            
            L∆∞u √Ω: H√†ng Copyright l√† b·∫Øt bu·ªôc ph·∫£i c√≥ cu·ªëi b·∫£ng.
            """
            
            # G·ª≠i request t·ªõi Gemini v·ªõi ·∫£nh
            response = self.gemini_model.generate_content([
                prompt,
                {"mime_type": "image/jpeg", "data": image_data}
            ])
            
            return response.text.strip()
            
        except Exception as e:
            logger.error(f"L·ªói khi tr√≠ch xu·∫•t th√¥ng s·ªë t·ª´ ·∫£nh: {e}")
            return ""
    
    def download_and_process_image(self, image_url: str, save_path: str, add_white_background: bool = False) -> bool:
        """
        T·∫£i xu·ªëng v√† x·ª≠ l√Ω ·∫£nh (chuy·ªÉn sang WebP, th√™m n·ªÅn tr·∫Øng n·∫øu c·∫ßn)
        
        Args:
            image_url: URL c·ªßa ·∫£nh
            save_path: ƒê∆∞·ªùng d·∫´n l∆∞u ·∫£nh
            add_white_background: C√≥ th√™m n·ªÅn tr·∫Øng kh√¥ng
            
        Returns:
            True n·∫øu th√†nh c√¥ng
        """
        try:
            response = self.safe_request(image_url)
            if not response:
                return False
            
            # M·ªü ·∫£nh
            original_image = Image.open(io.BytesIO(response.content))
            
            # Chuy·ªÉn sang RGBA n·∫øu c·∫ßn
            if original_image.mode != 'RGBA':
                original_image = original_image.convert('RGBA')
            
            processed_image = original_image
            
            # Th√™m n·ªÅn tr·∫Øng n·∫øu y√™u c·∫ßu
            if add_white_background:
                # T·∫°o ·∫£nh n·ªÅn tr·∫Øng c√πng k√≠ch th∆∞·ªõc
                white_background = Image.new('RGBA', original_image.size, (255, 255, 255, 255))
                # Gh√©p ·∫£nh g·ªëc l√™n n·ªÅn tr·∫Øng
                processed_image = Image.alpha_composite(white_background, original_image)
            
            # Chuy·ªÉn sang RGB ƒë·ªÉ l∆∞u WebP
            if processed_image.mode == 'RGBA':
                processed_image = processed_image.convert('RGB')
            
            # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # L∆∞u d∆∞·ªõi ƒë·ªãnh d·∫°ng WebP
            processed_image.save(save_path, 'WEBP', quality=85, optimize=True)
            
            logger.debug(f"ƒê√£ l∆∞u ·∫£nh: {save_path}")
            return True
            
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω ·∫£nh {image_url}: {e}")
            return False
    
    def process_series_parallel(self, series_list: List[Dict]) -> List[Dict]:
        """
        X·ª≠ l√Ω nhi·ªÅu series song song b·∫±ng ƒëa lu·ªìng
        
        Args:
            series_list: Danh s√°ch series c·∫ßn x·ª≠ l√Ω
            
        Returns:
            Danh s√°ch t·∫•t c·∫£ s·∫£n ph·∫©m t·ª´ c√°c series
        """
        all_products = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit c√°c task
            future_to_series = {
                executor.submit(self.extract_products_from_series, series['url']): series
                for series in series_list
            }
            
            # X·ª≠ l√Ω k·∫øt qu·∫£
            for future in as_completed(future_to_series):
                series = future_to_series[future]
                try:
                    products = future.result()
                    for product in products:
                        product['series_name'] = series['name']
                        product['series_image'] = series['image']
                    all_products.extend(products)
                    logger.info(f"Ho√†n th√†nh series: {series['name']} - {len(products)} s·∫£n ph·∫©m")
                except Exception as e:
                    logger.error(f"L·ªói khi x·ª≠ l√Ω series {series['name']}: {e}")
        
        return all_products
    
    def process_products_parallel(self, products_list: List[Dict]) -> List[Dict]:
        """
        X·ª≠ l√Ω chi ti·∫øt nhi·ªÅu s·∫£n ph·∫©m song song b·∫±ng ƒëa lu·ªìng
        
        Args:
            products_list: Danh s√°ch s·∫£n ph·∫©m c·∫ßn x·ª≠ l√Ω chi ti·∫øt
            
        Returns:
            Danh s√°ch s·∫£n ph·∫©m ƒë√£ x·ª≠ l√Ω chi ti·∫øt
        """
        detailed_products = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit c√°c task
            future_to_product = {
                executor.submit(self.extract_product_details, product['url'], product['code']): product
                for product in products_list
            }
            
            # X·ª≠ l√Ω k·∫øt qu·∫£
            for future in as_completed(future_to_product):
                base_product = future_to_product[future]
                try:
                    detailed_product = future.result()
                    if detailed_product:
                        # Merge th√¥ng tin c∆° b·∫£n v·ªõi chi ti·∫øt
                        detailed_product.update(base_product)
                        detailed_products.append(detailed_product)
                        logger.info(f"Ho√†n th√†nh chi ti·∫øt s·∫£n ph·∫©m: {detailed_product['code']}")
                except Exception as e:
                    logger.error(f"L·ªói khi x·ª≠ l√Ω chi ti·∫øt s·∫£n ph·∫©m {base_product['code']}: {e}")
        
        return detailed_products
    
    def process_images_parallel(self, products_list: List[Dict]) -> List[Dict]:
        """
        X·ª≠ l√Ω t·∫£i xu·ªëng v√† chuy·ªÉn ƒë·ªïi ·∫£nh song song
        
        Args:
            products_list: Danh s√°ch s·∫£n ph·∫©m c√≥ ch·ª©a URLs ·∫£nh
            
        Returns:
            Danh s√°ch s·∫£n ph·∫©m ƒë√£ c·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n ·∫£nh local
        """
        def download_product_images(product):
            """Download v√† x·ª≠ l√Ω t·∫•t c·∫£ ·∫£nh c·ªßa m·ªôt s·∫£n ph·∫©m"""
            product_code = product['code']
            series_name = product.get('series_name', 'unknown_series')
            
            # T·∫°o th∆∞ m·ª•c cho s·∫£n ph·∫©m
            product_dir = self.images_dir / series_name / product_code
            product_dir.mkdir(parents=True, exist_ok=True)
            
            updated_product = product.copy()
            
            # X·ª≠ l√Ω ·∫£nh s·∫£n ph·∫©m (th√™m n·ªÅn tr·∫Øng)
            if product['images']['product']:
                product_images_local = []
                for i, img_url in enumerate(product['images']['product']):
                    filename = f"product_{i+1}.webp"
                    save_path = product_dir / filename
                    if self.download_and_process_image(img_url, str(save_path), add_white_background=True):
                        product_images_local.append(str(save_path))
                updated_product['images']['product'] = product_images_local
            
            # X·ª≠ l√Ω ·∫£nh th√¥ng s·ªë k·ªπ thu·∫≠t
            if product['images']['specifications']:
                save_path = product_dir / "specifications.webp"
                if self.download_and_process_image(product['images']['specifications'], str(save_path)):
                    updated_product['images']['specifications'] = str(save_path)
            
            # X·ª≠ l√Ω ·∫£nh wiring diagram
            if product['images']['wiring_diagram']:
                save_path = product_dir / "wiring_diagram.webp"
                if self.download_and_process_image(product['images']['wiring_diagram'], str(save_path)):
                    updated_product['images']['wiring_diagram'] = str(save_path)
            
            # X·ª≠ l√Ω ·∫£nh dimensions
            if product['images']['dimensions']:
                save_path = product_dir / "dimensions.webp"
                if self.download_and_process_image(product['images']['dimensions'], str(save_path)):
                    updated_product['images']['dimensions'] = str(save_path)
            
            return updated_product
        
        processed_products = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit c√°c task download ·∫£nh
            future_to_product = {
                executor.submit(download_product_images, product): product
                for product in products_list
            }
            
            # X·ª≠ l√Ω k·∫øt qu·∫£
            for future in as_completed(future_to_product):
                original_product = future_to_product[future]
                try:
                    processed_product = future.result()
                    processed_products.append(processed_product)
                    logger.info(f"Ho√†n th√†nh x·ª≠ l√Ω ·∫£nh: {processed_product['code']}")
                except Exception as e:
                    logger.error(f"L·ªói khi x·ª≠ l√Ω ·∫£nh s·∫£n ph·∫©m {original_product['code']}: {e}")
        
        return processed_products
    
    def process_ai_tasks_parallel(self, products_list: List[Dict]) -> List[Dict]:
        """
        X·ª≠ l√Ω c√°c t√°c v·ª• AI (d·ªãch, tr√≠ch xu·∫•t th√¥ng s·ªë) song song
        
        Args:
            products_list: Danh s√°ch s·∫£n ph·∫©m c·∫ßn x·ª≠ l√Ω AI
            
        Returns:
            Danh s√°ch s·∫£n ph·∫©m ƒë√£ x·ª≠ l√Ω AI
        """
        def process_ai_for_product(product):
            """X·ª≠ l√Ω AI cho m·ªôt s·∫£n ph·∫©m"""
            updated_product = product.copy()
            
            # D·ªãch t√™n s·∫£n ph·∫©m sang ti·∫øng Vi·ªát
            if product['name']:
                vietnamese_name = self.translate_with_gemini(product['name'])
                updated_product['name_vietnamese'] = vietnamese_name
            
            # Tr√≠ch xu·∫•t th√¥ng s·ªë k·ªπ thu·∫≠t t·ª´ ·∫£nh
            if product['images']['specifications']:
                specs_table = self.extract_specs_from_image_with_gemini(
                    product['images']['specifications'], 
                    product['code']
                )
                updated_product['specifications_table'] = specs_table
            
            return updated_product
        
        if not self.gemini_model:
            logger.warning("Gemini AI kh√¥ng kh·∫£ d·ª•ng - b·ªè qua x·ª≠ l√Ω AI")
            return products_list
        
        processed_products = []
        
        with ThreadPoolExecutor(max_workers=min(self.max_workers, 3)) as executor:  # Gi·ªõi h·∫°n cho AI
            # Submit c√°c task AI
            future_to_product = {
                executor.submit(process_ai_for_product, product): product
                for product in products_list
            }
            
            # X·ª≠ l√Ω k·∫øt qu·∫£
            for future in as_completed(future_to_product):
                original_product = future_to_product[future]
                try:
                    processed_product = future.result()
                    processed_products.append(processed_product)
                    logger.info(f"Ho√†n th√†nh x·ª≠ l√Ω AI: {processed_product['code']}")
                except Exception as e:
                    logger.error(f"L·ªói khi x·ª≠ l√Ω AI s·∫£n ph·∫©m {original_product['code']}: {e}")
                    processed_products.append(original_product)  # Gi·ªØ b·∫£n g·ªëc n·∫øu l·ªói
        
        return processed_products
    
    def save_to_excel(self, products_list: List[Dict], filename: str):
        """
        L∆∞u d·ªØ li·ªáu s·∫£n ph·∫©m v√†o file Excel
        
        Args:
            products_list: Danh s√°ch s·∫£n ph·∫©m
            filename: T√™n file Excel
        """
        try:
            # Chu·∫©n b·ªã d·ªØ li·ªáu cho Excel
            excel_data = []
            
            for product in products_list:
                row = {
                    'M√£ s·∫£n ph·∫©m': product.get('code', ''),
                    'T√™n s·∫£n ph·∫©m (English)': product.get('name', ''),
                    'T√™n s·∫£n ph·∫©m (Ti·∫øng Vi·ªát)': product.get('name_vietnamese', ''),
                    'Series': product.get('series_name', ''),
                    'Nh√≥m s·∫£n ph·∫©m': product.get('group_name', ''),
                    'M√¥ t·∫£ t√≠nh nƒÉng': product.get('features', ''),
                    'URL s·∫£n ph·∫©m': product.get('url', ''),
                    '·∫¢nh s·∫£n ph·∫©m': ', '.join(product.get('images', {}).get('product', [])),
                    '·∫¢nh th√¥ng s·ªë k·ªπ thu·∫≠t': product.get('images', {}).get('specifications', ''),
                    '·∫¢nh s∆° ƒë·ªì ƒë·∫•u n·ªëi': product.get('images', {}).get('wiring_diagram', ''),
                    '·∫¢nh k√≠ch th∆∞·ªõc': product.get('images', {}).get('dimensions', ''),
                    'Th√¥ng s·ªë k·ªπ thu·∫≠t (HTML)': product.get('specifications_table', '')
                }
                excel_data.append(row)
            
            # T·∫°o DataFrame v√† l∆∞u Excel
            df = pd.DataFrame(excel_data)
            excel_path = self.excel_dir / filename
            
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='S·∫£n ph·∫©m', index=False)
                
                # T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh ƒë·ªô r·ªông c·ªôt
                worksheet = writer.sheets['S·∫£n ph·∫©m']
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
            
            logger.info(f"ƒê√£ l∆∞u d·ªØ li·ªáu v√†o Excel: {excel_path}")
            
        except Exception as e:
            logger.error(f"L·ªói khi l∆∞u Excel: {e}")
    
    def run_full_scraping(self, category_url: str, selected_series_indices: List[int] = None) -> Dict:
        """
        Ch·∫°y to√†n b·ªô quy tr√¨nh c√†o d·ªØ li·ªáu
        
        Args:
            category_url: URL trang danh m·ª•c
            selected_series_indices: Danh s√°ch index c·ªßa series c·∫ßn c√†o (None = t·∫•t c·∫£)
            
        Returns:
            Dict ch·ª©a th·ªëng k√™ k·∫øt qu·∫£
        """
        start_time = time.time()
        results = {
            'series_count': 0,
            'products_count': 0,
            'success_count': 0,
            'error_count': 0,
            'duration': 0
        }
        
        try:
            # B∆∞·ªõc 1: Tr√≠ch xu·∫•t series
            logger.info("üîç B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t danh s√°ch series...")
            all_series = self.extract_series_links(category_url)
            
            if not all_series:
                logger.error("Kh√¥ng t√¨m th·∫•y series n√†o!")
                return results
            
            # Ch·ªçn series c·∫ßn x·ª≠ l√Ω
            if selected_series_indices:
                selected_series = [all_series[i] for i in selected_series_indices if 0 <= i < len(all_series)]
            else:
                selected_series = all_series
            
            results['series_count'] = len(selected_series)
            logger.info(f"S·∫Ω x·ª≠ l√Ω {len(selected_series)} series")
            
            # B∆∞·ªõc 2: Tr√≠ch xu·∫•t s·∫£n ph·∫©m t·ª´ c√°c series (song song)
            logger.info("üì¶ B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t s·∫£n ph·∫©m t·ª´ c√°c series...")
            all_products = self.process_series_parallel(selected_series)
            results['products_count'] = len(all_products)
            
            if not all_products:
                logger.error("Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m n√†o!")
                return results
            
            logger.info(f"T√¨m th·∫•y t·ªïng c·ªông {len(all_products)} s·∫£n ph·∫©m")
            
            # B∆∞·ªõc 3: Tr√≠ch xu·∫•t chi ti·∫øt s·∫£n ph·∫©m (song song)
            logger.info("üîç B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t chi ti·∫øt s·∫£n ph·∫©m...")
            detailed_products = self.process_products_parallel(all_products)
            
            # B∆∞·ªõc 4: X·ª≠ l√Ω ·∫£nh (song song)
            logger.info("üñºÔ∏è B·∫Øt ƒë·∫ßu x·ª≠ l√Ω v√† t·∫£i xu·ªëng ·∫£nh...")
            processed_products = self.process_images_parallel(detailed_products)
            
            # B∆∞·ªõc 5: X·ª≠ l√Ω AI (song song)
            if self.gemini_model:
                logger.info("ü§ñ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω AI (d·ªãch v√† tr√≠ch xu·∫•t th√¥ng s·ªë)...")
                processed_products = self.process_ai_tasks_parallel(processed_products)
            
            # B∆∞·ªõc 6: L∆∞u d·ªØ li·ªáu
            logger.info("üíæ ƒêang l∆∞u d·ªØ li·ªáu...")
            
            # L∆∞u JSON
            json_path = self.output_dir / "fotek_products.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(processed_products, f, ensure_ascii=False, indent=2)
            
            # L∆∞u Excel
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            excel_filename = f"fotek_products_{timestamp}.xlsx"
            self.save_to_excel(processed_products, excel_filename)
            
            results['success_count'] = len(processed_products)
            results['error_count'] = results['products_count'] - results['success_count']
            
            logger.info("‚úÖ Ho√†n th√†nh c√†o d·ªØ li·ªáu!")
            
        except Exception as e:
            logger.error(f"L·ªói trong qu√° tr√¨nh c√†o d·ªØ li·ªáu: {e}")
            results['error_count'] += 1
        
        finally:
            results['duration'] = time.time() - start_time
        
        return results


if __name__ == "__main__":
    # V√≠ d·ª• s·ª≠ d·ª•ng
    print("=== FOTEK SCRAPER ===")
    print("Ch∆∞∆°ng tr√¨nh c√†o d·ªØ li·ªáu t·ª´ website Fotek.com.tw")
    print("Vui l√≤ng cung c·∫•p th√¥ng tin c·∫ßn thi·∫øt:")
    
    # Nh·∫≠p th√¥ng tin t·ª´ ng∆∞·ªùi d√πng
    category_url = input("Nh·∫≠p URL danh m·ª•c s·∫£n ph·∫©m: ").strip()
    if not category_url:
        category_url = "https://www.fotek.com.tw/en-gb/product-category/72"  # URL m·∫∑c ƒë·ªãnh
    
    gemini_api_key = input("Nh·∫≠p Gemini API key (t√πy ch·ªçn): ").strip()
    max_workers = input("S·ªë lu·ªìng x·ª≠ l√Ω (m·∫∑c ƒë·ªãnh 5): ").strip()
    max_workers = int(max_workers) if max_workers.isdigit() else 5
    
    # Kh·ªüi t·∫°o scraper
    scraper = FotekScraper(gemini_api_key=gemini_api_key, max_workers=max_workers)
    
    try:
        # B∆∞·ªõc 1: Tr√≠ch xu·∫•t c√°c series
        print(f"\nüîç ƒêang tr√≠ch xu·∫•t danh s√°ch series t·ª´: {category_url}")
        series_list = scraper.extract_series_links(category_url)
        
        if not series_list:
            print("‚ùå Kh√¥ng t√¨m th·∫•y series n√†o!")
            exit(1)
        
        print(f"‚úÖ T√¨m th·∫•y {len(series_list)} series")
        for i, series in enumerate(series_list, 1):
            print(f"  {i}. {series['name']}")
        
        # Cho ph√©p ng∆∞·ªùi d√πng ch·ªçn series c·ª• th·ªÉ
        choice = input(f"\nCh·ªçn series ƒë·ªÉ c√†o (1-{len(series_list)}, ho·∫∑c 'all' ƒë·ªÉ c√†o t·∫•t c·∫£): ").strip()
        
        selected_indices = []
        if choice.lower() == 'all':
            selected_indices = list(range(len(series_list)))
        else:
            try:
                # H·ªó tr·ª£ nhi·ªÅu l·ª±a ch·ªçn: 1,3,5 ho·∫∑c 1-5
                if ',' in choice:
                    indices = [int(x.strip()) - 1 for x in choice.split(',')]
                elif '-' in choice:
                    start, end = map(int, choice.split('-'))
                    indices = list(range(start - 1, end))
                else:
                    indices = [int(choice) - 1]
                
                selected_indices = [i for i in indices if 0 <= i < len(series_list)]
                
                if not selected_indices:
                    print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")
                    exit(1)
                    
            except ValueError:
                print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")
                exit(1)
        
        print(f"\nüöÄ B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu {len(selected_indices)} series...")
        
        # Ch·∫°y quy tr√¨nh c√†o d·ªØ li·ªáu ho√†n ch·ªânh
        results = scraper.run_full_scraping(category_url, selected_indices)
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        print("\n" + "="*50)
        print("üìä K·∫æT QU·∫¢ C√ÄO D·ªÆ LI·ªÜU")
        print("="*50)
        print(f"üè∑Ô∏è  S·ªë series x·ª≠ l√Ω: {results['series_count']}")
        print(f"üì¶ T·ªïng s·ªë s·∫£n ph·∫©m: {results['products_count']}")
        print(f"‚úÖ Th√†nh c√¥ng: {results['success_count']}")
        print(f"‚ùå L·ªói: {results['error_count']}")
        print(f"‚è±Ô∏è  Th·ªùi gian: {results['duration']:.2f} gi√¢y")
        
        if results['success_count'] > 0:
            print(f"\nüíæ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c: {scraper.output_dir}")
            print("üìÅ C·∫•u tr√∫c file:")
            print("   ‚îú‚îÄ‚îÄ fotek_products.json (D·ªØ li·ªáu JSON)")
            print("   ‚îú‚îÄ‚îÄ excel/ (File Excel)")
            print("   ‚îî‚îÄ‚îÄ images/ (·∫¢nh s·∫£n ph·∫©m theo series)")
            
            # Hi·ªÉn th·ªã m·ªôt s·ªë s·∫£n ph·∫©m ƒë·∫ßu ti√™n
            json_path = scraper.output_dir / "fotek_products.json"
            if json_path.exists():
                with open(json_path, 'r', encoding='utf-8') as f:
                    products = json.load(f)
                    if products:
                        print(f"\nüîç M·ªôt s·ªë s·∫£n ph·∫©m ƒë√£ c√†o:")
                        for i, product in enumerate(products[:5], 1):
                            name_vn = product.get('name_vietnamese', 'Ch∆∞a d·ªãch')
                            print(f"   {i}. {product['code']} - {name_vn}")
                        if len(products) > 5:
                            print(f"   ... v√† {len(products) - 5} s·∫£n ph·∫©m kh√°c")
        
        print("\nüéâ Ho√†n th√†nh ch∆∞∆°ng tr√¨nh!")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è D·ª´ng ch∆∞∆°ng tr√¨nh theo y√™u c·∫ßu ng∆∞·ªùi d√πng")
    except Exception as e:
        logger.error(f"L·ªói kh√¥ng mong mu·ªën: {e}")
        print(f"‚ùå L·ªói: {e}")